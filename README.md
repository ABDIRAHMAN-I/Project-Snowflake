# â„ï¸ Project Snowflake â€” Endâ€‘toâ€‘End ETL Pipeline with Airflow, Snowflake, and Terraform

A productionâ€‘grade, modular data pipeline that automates the ingestion, transformation, and loading of retailer data into Snowflake using **Python**, **Airflow**, **Terraform**, and **Docker**. Ideal for data engineers, DevOps engineers, and DataOps workflows.

---

## ğŸ“¸ Overview

**Project Snowflake** automates the entire data lifecycle:

1. ğŸ§¼ **Transform** raw CSV data
2. âœ‚ï¸ **Split** transformed data into logical tables
3. â˜ï¸ **Upload** tables to Amazon S3
4. ğŸ”ï¸ **Load** the final data into **Snowflake** using SQL scripts
5. âš™ï¸ All orchestrated by **Airflow DAGs**
6. ğŸ—ï¸ Infrastructure managed via **Terraform**
7. ğŸ³ Runs in a selfâ€‘contained **Docker Compose** environment

---

## ğŸš€ Architecture

```text
ğŸ“ CSV â†’ ğŸ Python (Transform) â†’ ğŸ“ Table CSVs â†’ â˜ï¸ S3 â†’ ğŸ§Š Snowflake
                â”‚                      â”‚             â”‚
                â”‚                      â”‚             â””â”€â”€â”€ via SQL in Airflow DAG
                â”‚                      â””â”€â”€â”€ upload_data_to_s3_bucket.py
                â””â”€â”€â”€ transform_data_script.py âœ split_data_into_tables.py

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Airflow    â”‚ â—€â”€â”€ Orchestrates pipeline
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ§° Tech Stack

| Tool      | Purpose                                     |
| --------- | ------------------------------------------- |
| Python    | ETL scripts                                 |
| Airflow   | DAG orchestration                           |
| Docker    | Containerisation and environment isolation  |
| Snowflake | Data warehouse                              |
| Terraform | Infrastructure provisioning (Snowflake IaC) |
| S3        | Cloud storage (intermediate staging)        |

### ğŸ—‚ï¸ Folder Structure

```text
Project-Snowflake/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ etl_snowflake_pipeline.py     â—€â”€â”€ Main DAG
â”‚   â”‚   â””â”€â”€ sql_scripts/load_data.sql     â—€â”€â”€ SQL for Snowflake loading
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ plugins/
â”‚
â”œâ”€â”€ assets/datasets/
â”‚   â”œâ”€â”€ Original_file/                    â—€â”€â”€ Raw CSVs
â”‚   â”œâ”€â”€ Transformed_full/                 â—€â”€â”€ Cleaned CSVs
â”‚   â””â”€â”€ Transformed_tables/               â—€â”€â”€ Split tables
â”‚
â”œâ”€â”€ python_scripts/
â”‚   â”œâ”€â”€ transform_data_script.py          â—€â”€â”€ Cleans and transforms data
â”‚   â”œâ”€â”€ split_data_into_tables.py         â—€â”€â”€ Splits data into logical tables
â”‚   â”œâ”€â”€ upload_data_to_s3_bucket.py       â—€â”€â”€ Uploads to S3
â”‚   â””â”€â”€ run_sql_script.py                 â—€â”€â”€ Executes SQL (now used in DAG)
â”‚
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf / variables.tf / modules/ â—€â”€â”€ IaC: schema, warehouse, tables, stage, integration
â”‚
â”œâ”€â”€ docker-compose.yaml                   â—€â”€â”€ Runs Airflow + Postgres
â”œâ”€â”€ Makefile                              â—€â”€â”€ CLI for common tasks
â”œâ”€â”€ .env                                  â—€â”€â”€ Env vars (used by Docker/Airflow)
â””â”€â”€ README.md
```

---

## âš™ï¸ How It Works

### ğŸ”„ DAG: `snowflake_etl_pipeline`

| Task ID               | Description                                            |
| --------------------- | ------------------------------------------------------ |
| `transform_data`      | Cleans raw CSV using Pandas                            |
| `split_into_tables`   | Splits cleaned data by entity (e.g. orders, products)  |
| `upload_to_s3`        | Uploads split CSVs to the S3 bucket                    |
| `load_into_snowflake` | Executes SQL script to load staged data into Snowflake |

Airflow handles retries, logging, and task chaining automatically.

---

## ğŸ Setup Instructions

### âœ… 1. Clone & Environment Setup

```bash
git clone https://github.com/yourusername/project-snowflake.git
cd project-snowflake
cp .env.example .env  # Edit AWS keys and other secrets
```

### ğŸ³ 2. Start Airflow with Docker Compose

```bash
make start
```

Runs Airflow, Postgres, and all required services.

### ğŸ› ï¸ 3. Provision Snowflake Infrastructure with Terraform

```bash
cd terraform
terraform init
terraform apply
```

Creates:

* Warehouse
* Database
* Schema
* Tables
* External Stage
* Integration

### ğŸŒ 4. Configure Airflow Connection in the UI

Open **Admin â†’ Connections** in the Airflow web UI (`http://localhost:8080`):

* **Connection ID**: `snowflake_default`
* **Connection Type**: `Snowflake`

Fill in credentials & **Extra** like:

```json
{
  "account": "your_account",
  "warehouse": "your_warehouse",
  "database": "your_database",
  "role": "your_role"
}
```

---

## ğŸ§ª Running the Pipeline

1. Place your raw CSV files into `assets/datasets/Original_file/`.
2. Trigger the **`snowflake_etl_pipeline`** DAG in the Airflow UI.
3. Monitor tasks as they progress: **transform â split â upload â load**.
4. View the final data in Snowflake.

---

## ğŸ“ˆ Future Enhancements

* ğŸ§ª Add data validation with **Great Expectations**
* ğŸ” Enable scheduling (daily/weekly)
* ğŸ“ Archive old processed files
* ğŸ” Add secret manager integration (AWS Secrets Manager or HashiCorp Vault)
* ğŸ“Š Connect **Power BI** or **Streamlit** for reporting
* ğŸ“¦ Convert this into a reusable **cookiecutter** template

---

## ğŸ‘¨â€ğŸ’» Author

**Abdirahman I.** â€” DataOps/DevOps Engineer
â€œI believe pipelines should be *reproducible, testable, and secure by design*.â€

---

## ğŸ“œ Licence

MIT Licence â€” free to use, modify, and learn from.
